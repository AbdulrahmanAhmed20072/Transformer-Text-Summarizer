# Transformer-Text-Summarizer
This project implements a Transformer-based text summarization model. It preprocesses text data, builds a vocabulary, tokenizes input, and trains an encoder-decoder Transformer model with self-attention mechanisms. The model generates concise summaries for input text and is optimized using masked loss and custom training steps.
